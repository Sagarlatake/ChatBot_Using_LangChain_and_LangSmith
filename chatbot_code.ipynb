{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:41.197817Z",
     "start_time": "2025-10-15T01:02:41.192215Z"
    }
   },
   "source": "# install Requirements.txt",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:41.224061Z",
     "start_time": "2025-10-15T01:02:41.216457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ],
   "id": "7f6a0381b553e292",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:41.245565Z",
     "start_time": "2025-10-15T01:02:41.235232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load env and keys for langsmith and groq\n",
    "# Prior to this you are expected to visit langsmith and groq websites and get your API keys ready\n",
    "load_dotenv()\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
    "        prompt=\"Enter your LangSmith API key (optional): \"\n",
    "    )\n",
    "if \"LANGSMITH_PROJECT\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(\n",
    "        prompt='Enter your LangSmith Project Name (default = \"default\"): '\n",
    "    )\n",
    "    if not os.environ.get(\"LANGSMITH_PROJECT\"):\n",
    "        os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\n",
    "\n",
    "\n",
    "\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n"
   ],
   "id": "600e6941c2b32c8a",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:41.261674Z",
     "start_time": "2025-10-15T01:02:41.255768Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain.chat_models import init_chat_model",
   "id": "ed25f5bde1630651",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:42.329653Z",
     "start_time": "2025-10-15T01:02:41.270125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# make sure you are accessing a valid model, feel free to change the model name\n",
    "model = init_chat_model(\"Llama-3.3-70B-Versatile\", model_provider=\"groq\")"
   ],
   "id": "d7b55966d7173f36",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:42.341868Z",
     "start_time": "2025-10-15T01:02:42.337522Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain_core.messages import HumanMessage",
   "id": "47662a3ad5f93d9c",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:43.108025Z",
     "start_time": "2025-10-15T01:02:42.352661Z"
    }
   },
   "cell_type": "code",
   "source": "model.invoke([HumanMessage(content=\"Hi! I'm sagar\")])",
   "id": "ad585aca27c9fcf0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Sagar! How are you today? Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 41, 'total_tokens': 66, 'completion_time': 0.061812261, 'prompt_time': 0.001883603, 'queue_time': 0.048243627, 'total_time': 0.063695864}, 'model_name': 'Llama-3.3-70B-Versatile', 'system_fingerprint': 'fp_155ab82e98', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2c81a48c-53ec-403b-8ea6-bf639f494dc0-0', usage_metadata={'input_tokens': 41, 'output_tokens': 25, 'total_tokens': 66})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:43.390514Z",
     "start_time": "2025-10-15T01:02:43.148407Z"
    }
   },
   "cell_type": "code",
   "source": "model.invoke([HumanMessage('what is my name?')])",
   "id": "90c9584bc647caad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't know your name. I'm a large language model, I don't have any information about you, including your name. I'm here to help answer your questions and provide information, but I don't have any personal knowledge about you. If you'd like to share your name, I'd be happy to chat with you!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 40, 'total_tokens': 109, 'completion_time': 0.138243841, 'prompt_time': 0.001804163, 'queue_time': 0.048619017, 'total_time': 0.140048004}, 'model_name': 'Llama-3.3-70B-Versatile', 'system_fingerprint': 'fp_34d416ee39', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--facc7b04-76a4-4449-8e6e-d11860a9e519-0', usage_metadata={'input_tokens': 40, 'output_tokens': 69, 'total_tokens': 109})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:43.485719Z",
     "start_time": "2025-10-15T01:02:43.473085Z"
    }
   },
   "cell_type": "code",
   "source": "# models response is not based on the previous conversation, as it is not taking previous conversation as input we will now pass entire conversation as input to model",
   "id": "8211687f1d8f775d",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:43.521986Z",
     "start_time": "2025-10-15T01:02:43.508449Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain_core.messages import AIMessage",
   "id": "2123567a54fd3a9a",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:43.674712Z",
     "start_time": "2025-10-15T01:02:43.549894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(\"Hi, My name is Sagar\"),\n",
    "        AIMessage(\"Hello Sagar! How can I assist today?\"),\n",
    "        HumanMessage(\"What is my name?\")\n",
    "    ]\n",
    ")"
   ],
   "id": "6a3b0f6218b8940e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Sagar.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 67, 'total_tokens': 74, 'completion_time': 0.019853532, 'prompt_time': 0.003250479, 'queue_time': 0.048984071, 'total_time': 0.023104011}, 'model_name': 'Llama-3.3-70B-Versatile', 'system_fingerprint': 'fp_155ab82e98', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--20b824cd-331c-466a-8986-2c72f5f10c40-0', usage_metadata={'input_tokens': 67, 'output_tokens': 7, 'total_tokens': 74})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Message persistence",
   "id": "9653e53654124705"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:43.732104Z",
     "start_time": "2025-10-15T01:02:43.715966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph"
   ],
   "id": "209043d40be0b759",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:43.767065Z",
     "start_time": "2025-10-15T01:02:43.752560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define new workflow\n",
    "workflow = StateGraph(state_schema=MessagesState)"
   ],
   "id": "129217ab5758f89e",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:43.799098Z",
     "start_time": "2025-10-15T01:02:43.784771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define function to call model\n",
    "def model_call(state: MessagesState):\n",
    "    resposne = model.invoke(state['messages'])\n",
    "    return {'messages': resposne}"
   ],
   "id": "fe7a6483c8deaefd",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:43.846779Z",
     "start_time": "2025-10-15T01:02:43.826526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define the nodes in graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", model_call)"
   ],
   "id": "2e60a34f3bff528e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1d22f20efd0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:43.894868Z",
     "start_time": "2025-10-15T01:02:43.879876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ],
   "id": "f659ae9196522ff",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:43.944679Z",
     "start_time": "2025-10-15T01:02:43.930078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create configurable for passing it to models, It supports multiple conversations threads with single application used by multiple users\n",
    "\n",
    "config = {\"configurable\":{\"thread_id\":\"abc123\"}}"
   ],
   "id": "6ef9b3a1f64a0b33",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:44.166981Z",
     "start_time": "2025-10-15T01:02:43.970566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_message = [HumanMessage('Hi, My name is Sagar')]\n",
    "output = app.invoke({\"messages\": input_message}, config)\n",
    "print(output)"
   ],
   "id": "b908dcd8b38e0f12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Hi, My name is Sagar', additional_kwargs={}, response_metadata={}, id='3778bbb4-adc9-45ae-829b-e59a7f4e8531'), AIMessage(content=\"Hello Sagar, it's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 42, 'total_tokens': 69, 'completion_time': 0.042871607, 'prompt_time': 0.002023039, 'queue_time': 0.04836656, 'total_time': 0.044894646}, 'model_name': 'Llama-3.3-70B-Versatile', 'system_fingerprint': 'fp_155ab82e98', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--7602f84b-7acc-4237-b213-95fcb8f5057f-0', usage_metadata={'input_tokens': 42, 'output_tokens': 27, 'total_tokens': 69})]}\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:44.211383Z",
     "start_time": "2025-10-15T01:02:44.198286Z"
    }
   },
   "cell_type": "code",
   "source": "output['messages'][-1].pretty_print()",
   "id": "8d387e6955c5aae3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Hello Sagar, it's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:44.470955Z",
     "start_time": "2025-10-15T01:02:44.260979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_message = [HumanMessage('what is my name?')]\n",
    "output = app.invoke({\"messages\": input_message}, config)\n",
    "print(output['messages'][-1].pretty_print())"
   ],
   "id": "5847093ecaaf63d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Your name is Sagar. You told me that when we started chatting.\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:44.529245Z",
     "start_time": "2025-10-15T01:02:44.515757Z"
    }
   },
   "cell_type": "code",
   "source": "# out models now started remembering things, now if we change config it will start a fresh chat",
   "id": "628bc07f6f6768c0",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:44.833293Z",
     "start_time": "2025-10-15T01:02:44.558057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = {\"configurable\": {\"thread_id\":\"abc12345\"}}\n",
    "input_message = [HumanMessage('what is my name?')]\n",
    "output = app.invoke({\"messages\": input_message}, config)\n",
    "print(output['messages'][-1].pretty_print())"
   ],
   "id": "9e501ab4c632fd26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "I don't have any information about your name. I'm a large language model, I don't have the ability to know your personal details or identity. Each time you interact with me, it's a new conversation and I don't retain any information from previous conversations. If you'd like to share your name with me, I'd be happy to chat with you and use it in our conversation!\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:02:45.038447Z",
     "start_time": "2025-10-15T01:02:44.874457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# let me check if model remembers out conversation with previous thread id\n",
    "config = {\"configurable\": {\"thread_id\":\"abc123\"}}\n",
    "input_message = [HumanMessage('Do you remember my name?')]\n",
    "output = app.invoke({\"messages\": input_message}, config)\n",
    "print(output['messages'][-1].pretty_print())"
   ],
   "id": "3a96afb7c5ea7136",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Yes, I remember your name. It's Sagar.\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T01:05:27.292032Z",
     "start_time": "2025-10-15T01:05:26.943327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "input_message = [HumanMessage('can you describe our conversation?')]\n",
    "# Async invocation:\n",
    "output = await app.ainvoke({\"messages\": input_message}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ],
   "id": "27cf5194ec167547",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "We've had a simple conversation so far. You introduced yourself, telling me that your name is Sagar. Then, you asked me to recall your name a few times, and I confirmed that I remembered it correctly. You also asked me to describe our conversation, which I'm doing now. That's the entirety of our conversation so far.\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5e75b63c3dae8ea4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
